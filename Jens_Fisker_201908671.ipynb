{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "In your own words, answer the following questions:\n",
    "\n",
    "**a:** In your opinion, what were the most important turning points in the history of deep learning?\n",
    "\n",
    "**Answer:** I would start the list of turning points off with the Darthmouth summer workshop from 1956 as many would consider this the birth of deep learning as a distinct field. Following this, 1958 also marks an important year, as American psychologist Frank Rosenblatt develops the perceptron.\n",
    "\n",
    "\n",
    "In 1989, LeCun, Y., Boser, B., Denker, J.S., et al. published the article Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation.\n",
    "On a small aside, in 2006 Geoffrey Hinton coins the term \"Deep Learning\", and the paper \"A fast learning algorithm for deep belief\" was published. This might not be a major breakthrough per se, but I would still consider it rather important in the context.\n",
    "While multiple points in time can be mentioned as important turning points in the history of deep learning, I will finish my answer with 2012, when Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton presented \"AlexNet\" and it wins the Imagenet Large Scale Visual Recognition Challenge. Furthermore, 2012 was also the year GPU trained networks started gaining traction, and the last decade, we can see the meteoric rise of Deep learning, and a lot of this is due to the technological advances in our GPU's.\n",
    "\n",
    "**b:** Explain the ADAM optimizer.\n",
    "\n",
    "**Answer:** Optimizers are methods used to find the parameters that minimize the loss function.\n",
    "Adam is an extension of Stochastic gradient descent, and is a way to add Adaptive moment estimation to the optimizer.\n",
    "Furthermore, Adam is a combination of two gradient descent methods; Momentum, and Root Mean Square Propagation (RSMP)\n",
    "\n",
    "**c:** Assume data input is a single channel 30x40 pixel image. First layer is a convolutional layer with 5 filters, with kernel size 3x2, step size (1,1) and padding='valid'. What are the output dimensions?\n",
    "\n",
    "**d:** Assuming ReLU activations and offsets, and that the last layer is softmax, how many parameters does this network have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
