{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#Imports used throughout the notebook\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Moving computations to the gpu if available\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1:\n",
    "### a: NLP\n",
    "Please discuss the recent trend of rapidly increasing sizes of NLP architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b: Transfer Learning\n",
    "Classify the following example of transfer learning. More exactly, what are the domains and tasks, and which are being changed?\n",
    "\n",
    "Source: Using a step counter to monitor exercise in healthy people.\n",
    "\n",
    "Target: Using a step counter to indicate recovery progression in a patient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c: Attention\n",
    "\n",
    "Assume dotproduct attention, and that the hidden states of the encoder layer are [0,1,4],[-1,1,2],[1,1,1],[2,1,1]. If the activation for the previous decoder is [0.1,1,-2], what is the attention-context vector?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d: Transformers\n",
    "\n",
    "Explain the 'positional encoding' step for transformers. Why is it done, how is it done?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e: Bounding box detection:\n",
    "Given a dataset with two classes; cats and dogs, and the following detections:\n",
    "\n",
    "TP = True positive\n",
    "FP = False positive\n",
    "\n",
    "cat_det = [TP, FP, TP, FP, TP]\n",
    "pred_scores_cat = [0.7, 0.3, 0.5, 0.6, 0.55]\n",
    "\n",
    "dog_det = [FP, TP, TP, FP, TP, TP]\n",
    "pred_scores_dog = [0.4, 0.35, 0.95, 0.5, 0.6, 0.7]\n",
    "\n",
    "There are in total 3 cats and 4 dogs in the images.\n",
    "\n",
    "Calculate the mean average precision (mAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f: Semantic segmentation - FCN 1:\n",
    "Given an image sized 1024x768x3 (width x height x channels), with 7 classes. What is the size of the target image if targets are one-hot encoded?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g: Semantic segmentation - FCN 2: \n",
    "What is a fully-convolutional network? When is it useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h:Residual Networks:\n",
    "Explain residual layers and their advantage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i: Intersection-Over-Union\n",
    "Calculate the intersection over union in for these four bounding-boxes and target bounding boxes:\n",
    "\n",
    "![](bounding-boxes.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j: Variational autoencoders:\n",
    "What are the strengths of a variational autoencoder (VAE) compared to an autoencoder (AE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2:\n",
    "Below is attached a script for generating a data set to learn translation of dates between multiple human readable and a machine readable format (ISO 8601). \n",
    "\n",
    "Task: Using an encoder-decoder setup, perform translation from human readable to machine readable formats. Please express the performance of your trained network in terms of average accuracy of the translated output (so, accuracy on a per-character basis). \n",
    "\n",
    "Restriction: we specifically demand that your presented solution does not include an attention layer. \n",
    "\n",
    " Despite this restriction, the task can be solved in numerous different ways. Here are some examples of solutions of similar problems, for inspiration: \n",
    "\n",
    "https://jscriptcoder.github.io/date-translator/Machine%20Translation%20with%20Attention%20model.html\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "Script for generating data set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from dateTrans_student1.py file from brightspace\n",
    "\"\"\"\n",
    "Created on Mon Oct 18 17:47:38 2021\n",
    "\n",
    "@author: au207178\n",
    "\"\"\"\n",
    "\n",
    "#https://www.kaggle.com/eswarchandt/neural-machine-translation-with-attention-dates\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "Faker.seed(101)\n",
    "random.seed(101)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "#%% pytorch dataset\n",
    "\n",
    "class datesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,locale='da',inputLength=40,outputLength=12, dataSetSize=100):\n",
    "        \n",
    "        self.inputLength=inputLength\n",
    "        self.outputLength=outputLength\n",
    "        self.length=dataSetSize\n",
    "        self.lan=locale\n",
    "        \n",
    "        self.FORMATS= ['short', # d/M/YY\n",
    "           'medium', # MMM d, YYY\n",
    "           'long', # MMMM dd, YYY\n",
    "           'full', # EEEE, MMM dd, YYY\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd/MM/YYY',\n",
    "           'EE d, MMM YYY',\n",
    "           'EEEE d, MMMM YYY']\n",
    "        \n",
    "\n",
    "        #generate vocabularies:\n",
    "        alphabet=sorted(tuple('abcdefghijklmnopqrstuvwxyzæøå'))\n",
    "        numbers=sorted(tuple('0123456789'))\n",
    "        symbols=['<SOS>','<EOS>',' ',',','.','/','-','<unk>', '<pad>'];\n",
    "        self.humanVocab=dict(zip(symbols+numbers+alphabet,\n",
    "                            list(range(len(symbols)+len(numbers)+len(alphabet)))))\n",
    "        self.machineVocab =dict(zip(symbols+numbers,list(range(len(symbols)+len(numbers)))))\n",
    "        self.invMachineVocab= {v: k for k, v in self.machineVocab.items()}\n",
    "\n",
    "    def string_to_int(self,string, length, vocab):\n",
    "        string = string.lower()\n",
    "        \n",
    "\n",
    "        if not len(string)+2<=length: #+2 to make room for SOS and EOS\n",
    "            print(len(string),string)\n",
    "            print('Length:',length)\n",
    "            \n",
    "            raise AssertionError()\n",
    "\n",
    "        \n",
    "        rep = list(map(lambda x: vocab.get(x, '<unk>'),string))\n",
    "        rep.insert(0,vocab['<SOS>']); rep.append(vocab['<EOS>']) #add start and of sequence\n",
    "        \n",
    "        if len(string) < length:\n",
    "            rep += [vocab['<pad>']] * (length - len(rep))\n",
    "        \n",
    "        return rep        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        dt = fake.date_object()\n",
    "\n",
    "        date = format_date(dt, format=random.choice(self.FORMATS), locale=self.lan)\n",
    "        human_readable = date.lower().replace(',', '')\n",
    "        machine_readable = dt.isoformat()\n",
    "        \n",
    "        humanEncoded=torch.LongTensor(self.string_to_int(human_readable,self.inputLength,self.humanVocab))\n",
    "        machineEncoded=torch.LongTensor(self.string_to_int(machine_readable,self.outputLength,self.machineVocab))\n",
    "        \n",
    "\n",
    "        \n",
    "        return human_readable, machine_readable, humanEncoded,machineEncoded\n",
    "\n",
    "e=datesDataset()\n",
    "human_readable, machine_readable, humanEncoded,machineEncoded=e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
