{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#Imports used throughout the notebook\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Moving computations to the gpu if available\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1:\n",
    "### a: NLP\n",
    "Please discuss the recent trend of rapidly increasing sizes of NLP architectures. \n",
    "\n",
    "### Answer:\n",
    "NLP architectures, especially in combination with transformers, have been found to scale exceptionally well with size. To my knowledge, this trend that larger models perform better is basically ubiqutous across all topics in NLP. The reason we see these models explode due to the sheer amount of available AND accesible data, combined with the massive advances in GPUs and TPUs in recent years. Thus, it has been seen that models with more parameters can learn more complex patterns and are better at generalizing from training data.\n",
    "\n",
    "While these improvements are generally considered to a step towards technological advancements, some people are concerned with the ethical and ecological dilemmas of such models.\n",
    "Firstly, training these humongous models are very costly with some articles claiming the training of GPT-4 emitted upwards of 15 metric tons of CO2, and others claiming estimates of 43,200kg CO2 emitted daily while running chatGPT - more than the average emissions from 30 people taking a transatlantic flight.\n",
    "\n",
    "Secondly, some people are scared that such intelligent models might eventually turn sentient if the trends continue. How does a model determine what is right and wrong, could this be abused? What if these models fall into the hands of the wrong people? These are questions shared by many individuals in todays world, and questions even people like Sam Altman, CEO of OpenAI, have talked about in podcasts and even hearings. \n",
    "\n",
    "I think there are a lot of opinions on this topic and its a very interesting ongoing debate. For this assignment though, I hope this was \"enough\" of a discussion, without deep diving into the topic completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b: Transfer Learning\n",
    "Classify the following example of transfer learning. More exactly, what are the domains and tasks, and which are being changed?\n",
    "\n",
    "Source: Using a step counter to monitor exercise in healthy people.\n",
    "\n",
    "Target: Using a step counter to indicate recovery progression in a patient\n",
    "\n",
    "### Answer:\n",
    "The domain D is a combination of the feature space and the marginal distribution. <br>\n",
    "In this case, the feature space \n",
    "$$\\chi_{s} = \\text{steps counted in healthy people}$$ \n",
    "$$\\chi_{t} = \\text{steps counted in patients}$$ \n",
    "and the marginal distribution\n",
    "$$P(\\chi_{s}) = \\mathbb{N}_0$$\n",
    "$$P(\\chi_{t}) = \\mathbb{N}_0$$\n",
    "While their distributions might be of the same form, its important to realise that recovering patients on avg. probably wont be having as many steps. This does not mean we can't use the information from the source domain to learn something about the target domain.<br>\n",
    "\n",
    "The task T is then a combination of possible labels (y), and an unknown conditional probability function.<br>\n",
    "Here, the possible labels arent explicit, but in this context, and with available information, I went with {exercising, not-exercising}. Other possible sets of labels could be:{fat, fit}, {rich, poor}, {old, young}, {male, female} etc. Its just about what information is available on the people providing step information, and I think this part of the exercise could have been worded better.<br>\n",
    "$$y_{s} = \\text{{exercising, not-exercising}}$$\n",
    "$$y_{t} = \\text{{recovered, not-recovered}}$$\n",
    "\n",
    "\n",
    "Lastly, for the conditional probability function\n",
    "$$P(y|\\chi_{s}) = \\text{predicting exercise levels in people}$$\n",
    "$$P(y|\\chi_{t}) = \\text{predicting recovery progression in people}$$\n",
    "\n",
    "Thus, following https://commons.wikimedia.org/w/index.php?curid=58812416 by By Emilie Morvant - Own work, CC BY-SA 4.0, we can see that we have the same source and target marginal distributions on $\\chi$, but the tasks are different between the source and target domains.<br>\n",
    "\n",
    "Meaning, we land in Inductive Transfer Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c: Attention\n",
    "\n",
    "Assume sdotproduct attention, and that the hidden states of the encoder layer are [0,1,4],[-1,1,2],[1,1,1],[2,1,1]. If the activation for the previous decoder is [0.1,1,-2], what is the attention-context vector?\n",
    "\n",
    "### Answer:\n",
    "sdot prodcut attention is given as<br>\n",
    "$$ a_{ij} = s^T_{i-1}h_j$$\n",
    "\n",
    "We have the hidden states (h) of the encoder given<br>\n",
    "$h_1 = [0, 1, 4]$<br>\n",
    "$h_2 = [-1, 1, 2]$<br>\n",
    "$h_3 = [1, 1, 1]$<br>\n",
    "$h_4 = [2, 1, 1]$<br>\n",
    "And the previous decoder activation $s_{i-1}$<br>\n",
    "$s_{i-1} = [0.1, 1, -2]$<br>\n",
    "Thus, we can calculate the sdot product $a_{ij}<br>\n",
    "$ a_{i1} = [0.1, 1, -2]^T \\sdot [0, 1, 4] = -7$<br>\n",
    "$ a_{i2} = [0.1, 1, -2]^T \\sdot [-1, 1, 2] = -3.1$<br>\n",
    "$ a_{i3} = [0.1, 1, -2]^T \\sdot [1, 1, 1] = -0.9$<br>\n",
    "$ a_{i4} = [0.1, 1, -2]^T \\sdot [2, 1, 1] = -0.8$<br>\n",
    "\n",
    "Using these, we can calculate teh attention weights $\\alpha_{ij}$.<br>\n",
    "$\\alpha_{ij}$ is given as<br>\n",
    "$$\\alpha_{ij} = \\frac{e^{a_{ij}}}{\\sum_{k}^{}e^{a_{ik}}}$$\n",
    "since we have calculated our $a_{ij}$'s, we start of with calculating $\\sum_{k}^{}e^{a_{ik}}$<br>\n",
    "$\\sum_{k}^{}e^{a_{ik}}=e^{-7}+e^{-3.1}+e^{-0.9}+e^{-0.8}=0.90185970821$<br>\n",
    "Thus, we get \n",
    "$$\\alpha_{i1} = \\frac{e^{-7}}{e^{0.902}} = 0.0003700547$$\n",
    "$$\\alpha_{i2} = \\frac{e^{-3.1}}{e^{0.902}} = 0.01828160879$$\n",
    "$$\\alpha_{i3} = \\frac{e^{-0.9}}{e^{0.902}} = 0.16499176618$$\n",
    "$$\\alpha_{i4} = \\frac{e^{-0.8}}{e^{0.902}} = 0.18234410171$$\n",
    "\n",
    "Now we can calculate the context vector $c_i$ given as<br>\n",
    "$$c_i=\\sum_{j}^{} \\alpha_{ij}h_j$$\n",
    "As\n",
    "$$c_i=0.0003700547 \\sdot [0, 1, 4] + 0.01828160879 \\sdot [-1, 1, 2] + 0.16499176618 \\sdot [1, 1, 1] + 0.18234410171 \\sdot [2, 1, 1] = [0.511398, 0.365988, 0.385379]$$\n",
    "A bit prettier:\n",
    "$$c_i=[0.511398, 0.365988, 0.385379]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d: Transformers\n",
    "\n",
    "Explain the 'positional encoding' step for transformers. Why is it done, how is it done?\n",
    "\n",
    "### Answer:\n",
    "Unlike RNNs or LSTMs, which inherently process data sequentially, transformers process input data in parallel. Thus, transformers need to incorporate the information given by the sequence order. This is why positional encoding is used. An example of two sentences that would be identical without positional encoding in transformers could be \"I did very well in Deep Learning\" and \"very well in Deep Learning I did\".\n",
    "\n",
    "One way to do it, as in the original paper, a positional encoding tensor were added to the inputs, with the same dimension, such that they can be summed:\n",
    "$$ \\Chi \\rightarrow \\Chi + PE$$ \n",
    "Where PE is defined as<br>\n",
    "$$PE(pos,2i) = \\sin(\\frac{pos}{10000 \\sdot \\frac{2i}{d_{model}}})$$\n",
    "$$PE(pos,2i+1) = \\cos(\\frac{pos}{10000 \\sdot \\frac{2i}{d_{model}}})$$\n",
    "Where pos is the position and i is the dimension.<br>\n",
    "This is a \"relative encoding\" approach, as the values of the encoding does not scale with input length.<br>\n",
    "The discussion on how to approach encoding is not yet over, so while this implementation showcased additive encoding, it might not be the best approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e: Bounding box detection:\n",
    "Given a dataset with two classes; cats and dogs, and the following detections:\n",
    "\n",
    "TP = True positive\n",
    "FP = False positive\n",
    "\n",
    "cat_det = [TP, FP, TP, FP, TP]\n",
    "pred_scores_cat = [0.7, 0.3, 0.5, 0.6, 0.55]\n",
    "\n",
    "dog_det = [FP, TP, TP, FP, TP, TP]\n",
    "pred_scores_dog = [0.4, 0.35, 0.95, 0.5, 0.6, 0.7]\n",
    "\n",
    "There are in total 3 cats and 4 dogs in the images.\n",
    "\n",
    "Calculate the mean average precision (mAP)\n",
    "\n",
    "### Answer:\n",
    "mAP is defined as\n",
    "$$mAP = \\frac{1}{N} \\sum_{i=1}^{N}AP_i$$\n",
    "Where AP is the average precision for each class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f: Semantic segmentation - FCN 1:\n",
    "Given an image sized 1024x768x3 (width x height x channels), with 7 classes. What is the size of the target image if targets are one-hot encoded?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g: Semantic segmentation - FCN 2: \n",
    "What is a fully-convolutional network? When is it useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h:Residual Networks:\n",
    "Explain residual layers and their advantage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i: Intersection-Over-Union\n",
    "Calculate the intersection over union in for these four bounding-boxes and target bounding boxes:\n",
    "\n",
    "![](bounding-boxes.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j: Variational autoencoders:\n",
    "What are the strengths of a variational autoencoder (VAE) compared to an autoencoder (AE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2:\n",
    "Below is attached a script for generating a data set to learn translation of dates between multiple human readable and a machine readable format (ISO 8601). \n",
    "\n",
    "Task: Using an encoder-decoder setup, perform translation from human readable to machine readable formats. Please express the performance of your trained network in terms of average accuracy of the translated output (so, accuracy on a per-character basis). \n",
    "\n",
    "Restriction: we specifically demand that your presented solution does not include an attention layer. \n",
    "\n",
    " Despite this restriction, the task can be solved in numerous different ways. Here are some examples of solutions of similar problems, for inspiration: \n",
    "\n",
    "https://jscriptcoder.github.io/date-translator/Machine%20Translation%20with%20Attention%20model.html\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "Script for generating data set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from dateTrans_student1.py file from brightspace\n",
    "\"\"\"\n",
    "Created on Mon Oct 18 17:47:38 2021\n",
    "\n",
    "@author: au207178\n",
    "\"\"\"\n",
    "\n",
    "#https://www.kaggle.com/eswarchandt/neural-machine-translation-with-attention-dates\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "Faker.seed(101)\n",
    "random.seed(101)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "#%% pytorch dataset\n",
    "\n",
    "class datesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,locale='da',inputLength=40,outputLength=12, dataSetSize=100):\n",
    "        \n",
    "        self.inputLength=inputLength\n",
    "        self.outputLength=outputLength\n",
    "        self.length=dataSetSize\n",
    "        self.lan=locale\n",
    "        \n",
    "        self.FORMATS= ['short', # d/M/YY\n",
    "           'medium', # MMM d, YYY\n",
    "           'long', # MMMM dd, YYY\n",
    "           'full', # EEEE, MMM dd, YYY\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd/MM/YYY',\n",
    "           'EE d, MMM YYY',\n",
    "           'EEEE d, MMMM YYY']\n",
    "        \n",
    "\n",
    "        #generate vocabularies:\n",
    "        alphabet=sorted(tuple('abcdefghijklmnopqrstuvwxyzæøå'))\n",
    "        numbers=sorted(tuple('0123456789'))\n",
    "        symbols=['<SOS>','<EOS>',' ',',','.','/','-','<unk>', '<pad>'];\n",
    "        self.humanVocab=dict(zip(symbols+numbers+alphabet,\n",
    "                            list(range(len(symbols)+len(numbers)+len(alphabet)))))\n",
    "        self.machineVocab =dict(zip(symbols+numbers,list(range(len(symbols)+len(numbers)))))\n",
    "        self.invMachineVocab= {v: k for k, v in self.machineVocab.items()}\n",
    "\n",
    "    def string_to_int(self,string, length, vocab):\n",
    "        string = string.lower()\n",
    "        \n",
    "\n",
    "        if not len(string)+2<=length: #+2 to make room for SOS and EOS\n",
    "            print(len(string),string)\n",
    "            print('Length:',length)\n",
    "            \n",
    "            raise AssertionError()\n",
    "\n",
    "        \n",
    "        rep = list(map(lambda x: vocab.get(x, '<unk>'),string))\n",
    "        rep.insert(0,vocab['<SOS>']); rep.append(vocab['<EOS>']) #add start and of sequence\n",
    "        \n",
    "        if len(string) < length:\n",
    "            rep += [vocab['<pad>']] * (length - len(rep))\n",
    "        \n",
    "        return rep        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        dt = fake.date_object()\n",
    "\n",
    "        date = format_date(dt, format=random.choice(self.FORMATS), locale=self.lan)\n",
    "        human_readable = date.lower().replace(',', '')\n",
    "        machine_readable = dt.isoformat()\n",
    "        \n",
    "        humanEncoded=torch.LongTensor(self.string_to_int(human_readable,self.inputLength,self.humanVocab))\n",
    "        machineEncoded=torch.LongTensor(self.string_to_int(machine_readable,self.outputLength,self.machineVocab))\n",
    "        \n",
    "\n",
    "        \n",
    "        return human_readable, machine_readable, humanEncoded,machineEncoded\n",
    "\n",
    "e=datesDataset()\n",
    "human_readable, machine_readable, humanEncoded,machineEncoded=e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('20 nov. 1989',\n",
       " '1989-11-20',\n",
       " tensor([ 0, 11,  9,  2, 32, 33, 40,  4,  2, 10, 18, 17, 18,  1,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8]),\n",
       " tensor([ 0, 10, 18, 17, 18,  6, 10, 10,  6, 11,  9,  1]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doing some basic testing to get a feel for the dataset\n",
    "\n",
    "#e[x][0] are the human readable dates, e[x][1] are the machine readable dates. [x][2] encodes [x][0] and [x][3] encodes [x][1]\n",
    "e[0]\n",
    "#e.machineVocab\n",
    "#e.humanVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network\n",
    "#I spent a couple of hours trying with an LSTM implementation, but in the end I was having too many issues with the architecture, so, i ended up with a GRU implementation instead.\n",
    "#The good thing about using GRU as the architecture, is its faster convergence compared to LSTM, and it's also less prone to overfitting.\n",
    "#I used the tutorial from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html as a guideline, both when trying with the LSTM and when implementing the GRU.\n",
    "\n",
    "#https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "        \n",
    "#https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0) #SOS token is 0 in the vocabs\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(40): #Max length of input sequence is 40\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "            \n",
    "        \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Net, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.4):        \n",
    "        # src: source sequence, trg: target sequence\n",
    "         #Teacher forcing ratio is the probability of using teacher forcing, I tried different values, eg. 0.05, 0.1, 0.2, 0.5, 0.9, 1.0\n",
    "         #but 0.4 seemed fine, while also not making the model rely on it too much.\n",
    "        batch_size = src.size(0)\n",
    "        max_len = trg.size(1)  #12 (len of longest output sequence)\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings #19 (len of machine vocab)\n",
    "        \n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # encode the source sentence\n",
    "        encoder_output, hidden = self.encoder(src) #As we're not doing attention, the encoder output is actually not used\n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = torch.tensor([0]*batch_size).to(self.device)  # assuming <sos> token is 0\n",
    "        input = input.unsqueeze(1)  # add batch dimension\n",
    "        \n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder.forward_step(input, hidden) #output is the logits\n",
    "            outputs[:, t] = output.squeeze(1) #Save the logits in the outputs tensor\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.squeeze(1).max(1)[1] #Get the index of the highest logit\n",
    "            input = (trg[:, t] if teacher_force else top1).unsqueeze(1) #If teacher forcing, use the actual target, else use the highest logit in the next iteration\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "X, y, X_enc, y_enc = zip(*[e[i] for i in range(e.length)])\n",
    "\n",
    "#Making the encoded data into tensors\n",
    "X_enc = torch.stack(X_enc)\n",
    "y_enc = torch.stack(y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jens\\AppData\\Local\\Temp\\ipykernel_6728\\3526470852.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(X_enc, dtype=torch.long).to(device)\n",
      "C:\\Users\\Jens\\AppData\\Local\\Temp\\ipykernel_6728\\3526470852.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(y_enc, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train and test sets\n",
    "#Defining the inputs and targets, creating the dataset\n",
    "inputs = torch.tensor(X_enc, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(y_enc, dtype=torch.long).to(device)\n",
    "dataset = torch.utils.data.TensorDataset(inputs, targets)\n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    "#Batch size\n",
    "batch_size = 16 #Tested with bs 4, 16, 24, 32, 64. 16 seemed to be the best, but 24 and 32 were also fine.\n",
    "\n",
    "#Splitting the dataset into training, validation and test sets\n",
    "train_indices, temp_indices = train_test_split(indices, test_size=0.2, random_state=42) #80% training, 20% to be split below\n",
    "valid_indices, test_inddices = train_test_split(temp_indices, test_size=0.5, random_state=42) #10% validation, 10% test\n",
    "\n",
    "\n",
    "#Create subsets of the data using the split indices\n",
    "train_data = torch.utils.data.Subset(dataset, train_indices)\n",
    "valid_data = torch.utils.data.Subset(dataset, valid_indices)\n",
    "test_data = torch.utils.data.Subset(dataset, test_inddices)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_dl = DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc 0 : 0.11875\n",
      "Train loss 0 : tensor(14.7818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 0 : 2.901568651199341\n",
      "Val acc 0 : 0.14583333333333334\n",
      "Train acc 50 : 0.43645833333333334\n",
      "Train loss 50 : tensor(10.1085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 50 : 1.9745641946792603\n",
      "Val acc 50 : 0.4479166666666667\n",
      "Train acc 100 : 0.4479166666666667\n",
      "Train loss 100 : tensor(9.0248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 100 : 1.756047010421753\n",
      "Train acc 150 : 0.4875\n",
      "Train loss 150 : tensor(8.5627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 150 : 1.7199493646621704\n",
      "Val acc 150 : 0.53125\n",
      "Train acc 200 : 0.559375\n",
      "Train loss 200 : tensor(7.9359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 200 : 1.4749699831008911\n",
      "Train acc 250 : 0.6114583333333333\n",
      "Train loss 250 : tensor(7.1089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 250 : 0.625\n",
      "Train acc 350 : 0.6864583333333333\n",
      "Train loss 350 : tensor(5.9997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 350 : 1.1982444524765015\n",
      "Val acc 350 : 0.671875\n",
      "Train acc 400 : 0.6895833333333333\n",
      "Train loss 400 : tensor(5.6454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 400 : 0.6927083333333334\n",
      "Train acc 450 : 0.7010416666666667\n",
      "Train loss 450 : tensor(5.3352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 450 : 1.093652367591858\n",
      "Train acc 500 : 0.7114583333333333\n",
      "Train loss 500 : tensor(5.3036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 500 : 1.0750452280044556\n",
      "Val acc 500 : 0.7395833333333334\n",
      "Train acc 550 : 0.7375\n",
      "Train loss 550 : tensor(5.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 550 : 1.0521665811538696\n",
      "Train acc 600 : 0.75625\n",
      "Train loss 600 : tensor(4.8340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 600 : 1.0252037048339844\n",
      "Val acc 600 : 0.78125\n",
      "Train acc 650 : 0.7739583333333333\n",
      "Train loss 650 : tensor(4.6653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 650 : 1.0037099123001099\n",
      "Train acc 700 : 0.7854166666666667\n",
      "Train loss 700 : tensor(4.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 700 : 0.9986371397972107\n",
      "Val acc 700 : 0.8177083333333334\n",
      "Train acc 750 : 0.7885416666666667\n",
      "Train loss 750 : tensor(4.3457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 750 : 0.9896564483642578\n",
      "Train acc 800 : 0.8114583333333333\n",
      "Train loss 800 : tensor(4.1921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 800 : 0.9822566509246826\n",
      "Train acc 850 : 0.8260416666666667\n",
      "Train loss 850 : tensor(4.0083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss 850 : 0.9776142835617065\n",
      "Train acc 900 : 0.8385416666666666\n",
      "Train loss 900 : tensor(3.8529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 900 : 0.8541666666666666\n",
      "Train acc 950 : 0.8552083333333333\n",
      "Train loss 950 : tensor(3.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 950 : 0.8645833333333334\n",
      "Train acc 1000 : 0.8666666666666667\n",
      "Train loss 1000 : tensor(3.4964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1050 : 0.8864583333333333\n",
      "Train loss 1050 : tensor(3.3152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1050 : 0.8854166666666666\n",
      "Train acc 1100 : 0.8947916666666667\n",
      "Train loss 1100 : tensor(3.1408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1100 : 0.8958333333333334\n",
      "Train acc 1150 : 0.9072916666666667\n",
      "Train loss 1150 : tensor(2.9969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1150 : 0.9270833333333334\n",
      "Train acc 1200 : 0.91875\n",
      "Train loss 1200 : tensor(2.8574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1250 : 0.9302083333333333\n",
      "Train loss 1250 : tensor(2.6904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1250 : 0.9375\n",
      "Train acc 1300 : 0.9375\n",
      "Train loss 1300 : tensor(2.5597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1350 : 0.9416666666666667\n",
      "Train loss 1350 : tensor(2.4314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1350 : 0.953125\n",
      "Train acc 1400 : 0.9552083333333333\n",
      "Train loss 1400 : tensor(2.3210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1450 : 0.9604166666666667\n",
      "Train loss 1450 : tensor(2.1846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1500 : 0.9729166666666667\n",
      "Train loss 1500 : tensor(2.0555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1500 : 0.9583333333333334\n",
      "Train acc 1550 : 0.975\n",
      "Train loss 1550 : tensor(1.9942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1550 : 0.984375\n",
      "Train acc 1600 : 0.978125\n",
      "Train loss 1600 : tensor(1.9046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1650 : 0.990625\n",
      "Train loss 1650 : tensor(1.7974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1700 : 0.9927083333333333\n",
      "Train loss 1700 : tensor(1.7097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1700 : 0.9895833333333334\n",
      "Train acc 1750 : 0.99375\n",
      "Train loss 1750 : tensor(1.6500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1800 : 0.9958333333333333\n",
      "Train loss 1800 : tensor(1.5887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val acc 1800 : 1.0\n",
      "Train loss 1850 : tensor(1.5418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1900 : 0.9979166666666667\n",
      "Train loss 1900 : tensor(1.4939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train acc 1950 : 1.0\n",
      "Train loss 1950 : tensor(1.4556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train loss 2000 : tensor(1.4223, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size=48, hidden_size=512).to(device) #tested hidden size with 64, 128, 256. 128 performed best in test: 75 vs 80% accuracy\n",
    "decoder = Decoder(hidden_size=512, output_size=19).to(device)\n",
    "net = Net(encoder, decoder, device).to(device)\n",
    "#Optimizer and loss function\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) #tested with lr 0.005, 0.002, 0.001, 0.0005, 0.0001. - 0.001 performed best\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#Training\n",
    "nEpochs = 2001\n",
    "best_loss = 1000\n",
    "best_acc = 0\n",
    "best_valid_loss = 1000\n",
    "best_valid_acc = 0\n",
    "\n",
    "mod = 50 #Modulus for printing the best results\n",
    "\n",
    "for iEpoch in range(nEpochs):\n",
    "    net.train()\n",
    "    totLoss=0\n",
    "    cor_pred = 0\n",
    "    tot_pred = 0\n",
    "    cor_pred_val = 0\n",
    "    tot_pred_val = 0\n",
    "    for xbatch,ybatch in train_dl:\n",
    "        xbatch=xbatch.to(device=device)\n",
    "        ybatch=ybatch.to(device=device)\n",
    "\n",
    "        y_pred = net(src=xbatch, trg=ybatch)  \n",
    "        y_pred = y_pred.permute(0, 2, 1) ## permute to match CrossEntropyLoss input of (Batchsize, num_classes, seq_len)\n",
    "        \n",
    "        #Next section is to calculate the performance of average accuracy of the translated output\n",
    "        #Convert the logits to predictions\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        predictions = torch.argmax(softmax(y_pred), dim=1)\n",
    "        \n",
    "        # Mask to filter out padding tokens in the targets\n",
    "        mask = ybatch != 8\n",
    "        # Use the mask to filter out padding in both predictions and targets\n",
    "        predictions_masked = torch.masked_select(predictions, mask)\n",
    "        targets_masked = torch.masked_select(ybatch, mask)\n",
    "\n",
    "        # Correct predictions\n",
    "        cor_pred += (predictions_masked == targets_masked).sum().item()\n",
    "        \n",
    "        # Total predictions in batch\n",
    "        tot_pred += targets_masked.size(0)\n",
    "\n",
    "        #Back to Calculating loss like normal\n",
    "        loss_val = loss(y_pred, ybatch.long())\n",
    "        totLoss+=loss_val\n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        net.zero_grad()\n",
    "    \n",
    "        loss_val.backward()\n",
    "\n",
    "        #gradient clipping to deal with exploding gradients.\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(),10, norm_type=2.0)\n",
    "        optimizer.step()\n",
    "    accuracy_train = cor_pred / tot_pred\n",
    "    if (accuracy_train > best_acc) & (iEpoch % mod == 0):\n",
    "        best_acc = accuracy_train\n",
    "        print('Train acc', iEpoch,\":\", best_acc)\n",
    "        \n",
    "    if (totLoss < best_loss) & (iEpoch % mod == 0):\n",
    "        best_loss = totLoss\n",
    "        print('Train loss',iEpoch,\":\", totLoss)\n",
    "    \n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    valid_loss_sum = 0.0\n",
    "    correct_predictions = 0\n",
    "    #For this i just reused the code from training, some naming might be weird\n",
    "    for inputs, labels in valid_dl:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs, labels)\n",
    "        outputs = outputs.permute(0, 2, 1)\n",
    "        batch_loss = loss(outputs, labels.long())\n",
    "        valid_loss_sum += batch_loss.item()\n",
    "        \n",
    "        #Convert the logits to predictions\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        predictions = torch.argmax(softmax(y_pred), dim=1)\n",
    "        \n",
    "        # Mask for non-padding tokens in the targets\n",
    "        mask = ybatch != 8  # Create a mask for non-padding tokens\n",
    "        # Use the mask to filter out padding in both predictions and targets\n",
    "        predictions_masked = torch.masked_select(predictions, mask)\n",
    "        targets_masked = torch.masked_select(ybatch, mask)\n",
    "\n",
    "        # Correct predictions\n",
    "        cor_pred_val += (predictions_masked == targets_masked).sum().item()\n",
    "        \n",
    "        # Total predictions in batch\n",
    "        tot_pred_val += targets_masked.size(0)\n",
    "        \n",
    "    # Calculate accuracy\n",
    "    valid_acc = cor_pred_val / tot_pred_val\n",
    "\n",
    "    valid_loss = valid_loss_sum / len(valid_dl)\n",
    "\n",
    "    if (valid_loss < best_valid_loss) & (iEpoch % mod == 0):\n",
    "        best_valid_loss = valid_loss\n",
    "        netImage = net.state_dict()\n",
    "        print('Val loss', iEpoch,\":\", best_valid_loss)\n",
    "        bestPred = outputs  # Be cautious about overwriting bestPred every epoch\n",
    "    if (valid_acc > best_valid_acc) & (iEpoch % mod == 0):\n",
    "        best_valid_acc = valid_acc\n",
    "        netImage = net.state_dict()\n",
    "        print('Val acc', iEpoch,\":\", best_valid_acc)\n",
    "        bestPred = outputs  # Be cautious about overwriting bestPred every epoch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated best loss\n",
      "Test loss: 1.410487413406372\n",
      "Test acc: 0.775\n"
     ]
    }
   ],
   "source": [
    "#Some testing\n",
    "best_test_loss = 1000    \n",
    "net.eval()  # Set the model to evaluation mode\n",
    "test_loss_sum = 0.0\n",
    "best_test_acc = 0\n",
    "cor_pred_test = 0\n",
    "tot_pred_test = 0\n",
    "for inputs, labels in test_dl:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net(inputs, labels)\n",
    "    outputs = outputs.permute(0, 2, 1)\n",
    "    batch_loss = loss(outputs, labels.long())\n",
    "    test_loss_sum += batch_loss.item()\n",
    "    \n",
    "    #Convert the logits to predictions\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    predictions = torch.argmax(softmax(outputs), dim=1)\n",
    "    \n",
    "    # Mask for non-padding tokens in the targets\n",
    "    mask = labels != 8  # Create a mask for non-padding tokens\n",
    "    # Use the mask to filter out padding in both predictions and targets\n",
    "    predictions_masked = torch.masked_select(predictions, mask)\n",
    "    targets_masked = torch.masked_select(labels, mask)\n",
    "\n",
    "    # Correct predictions\n",
    "    cor_pred_test += (predictions_masked == targets_masked).sum().item()\n",
    "    \n",
    "    # Total predictions in batch\n",
    "    tot_pred_test += targets_masked.size(0)\n",
    "        \n",
    "    # Calculate accuracy\n",
    "test_acc = cor_pred_test / tot_pred_test\n",
    "\n",
    "test_loss = test_loss_sum / len(test_dl)\n",
    "\n",
    "if test_loss < best_test_loss:\n",
    "    best_loss = test_loss\n",
    "    netImage = net.state_dict()\n",
    "    print('updated best loss')\n",
    "    bestPred = outputs  # Be cautious about overwriting bestPred every epoch\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation\n",
    "So, we converge towards 1 in validation accuracy pretty fast, especially if we tune the hyperparameters a bit towards more hidden layers. The validation loss seems to converge around 1.1 and the training loss around 1.2x. With validation and training accuracy reaching 1.0 before 150 epochs.\n",
    "\n",
    "## Test results\n",
    "So a test accuracy of 80% was around the best i achieved (the highest i saw was 80,3). This was tested with different hyperparameters. It can be seen in the comments which values i tested with, but i tested on hidden_size, lr in the optimizer, and different batch sizes.\n",
    "If I'd been able to implement the logic using LSTM, I suspect i would've gotten better accuracy, however, time was running from me. Furthermore, with the use of attention we also would expect significant improvement to the accuracy.\n",
    "Maybe it could also have been interesting to look at the Date-level Accuracy, instead of only the Character accuracy.\n",
    "Overall, I think we would like higher accuracy, as the task, on an intuition level, does not seem that hard. I am not entirely sure why I was only able to achieve 80% accuracy. Maybe I'm overfitting, however, when I was running fewer epochs, the results were worse overall.\n",
    "Finally, I tried running 2000 epochs a few times, without any improvements. Also running with learning rates of down to 0.000001 without improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
